{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer trained and saved as 'bpe_tokenizer.json'\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load dataset from JSON file\n",
    "def load_json_dataset(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data = load_json_dataset(\"dataset.json\")\n",
    "\n",
    "# Extract pseudocode and C++ code\n",
    "pseudocode_texts = [item[\"pseudocode\"] for item in data]\n",
    "code_texts = [item[\"code\"] for item in data]\n",
    "\n",
    "# Initialize BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Train the tokenizer on both pseudocode and code\n",
    "trainer = trainers.BpeTrainer(vocab_size=8000,min_frequency=5, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "tokenizer.train_from_iterator(pseudocode_texts + code_texts, trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "print(\"Tokenizer trained and saved as 'bpe_tokenizer.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "\n",
    "# Load trained BPE tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Enable padding and truncation\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "tokenizer.enable_truncation(max_length=256)\n",
    "\n",
    "# Load dataset\n",
    "with open(\"dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = []\n",
    "for item in dataset:\n",
    "    pseudocode_tokens = tokenizer.encode(item[\"pseudocode\"]).ids\n",
    "    code_tokens = tokenizer.encode(item[\"code\"]).ids\n",
    "    tokenized_dataset.append({\"input\": pseudocode_tokens, \"output\": code_tokens})\n",
    "\n",
    "# Save tokenized dataset\n",
    "with open(\"tokenized_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenized_dataset, f)\n",
    "\n",
    "print(\"Tokenized dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded dataset successfully created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load tokenized dataset\n",
    "with open(\"tokenized_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenized_dataset = json.load(f)\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = [torch.tensor(d[\"input\"]) for d in tokenized_dataset]\n",
    "output_ids = [torch.tensor(d[\"output\"]) for d in tokenized_dataset]\n",
    "\n",
    "# Pad sequences to max length (256)\n",
    "padded_inputs = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "padded_outputs = pad_sequence(output_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(padded_inputs, padded_outputs)\n",
    "\n",
    "# DataLoader with optimized batch size\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(\"Padded dataset successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Define Transformer Model\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=8000, embed_dim=128, hidden_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomTransformer().to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train(model, dataloader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():  # Mixed Precision\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.view(-1, 8000), targets.view(-1))\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Train Model\n",
    "train(model, dataloader)\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
